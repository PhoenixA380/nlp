{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing requirements and creating session variables\n",
    "([Go to top](#Lab-6.1:-Implementing-Topic-Modeling-with-Amazon-Comprehend))\n",
    "\n",
    "In this section, you will update and install the packages that you will use in the notebook. You will also create the session variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "# Client and session information\n",
    "comprehend_client = boto3.client(service_name='comprehend')\n",
    "\n",
    "\n",
    "# Constants for the S3 bucket and input data file\n",
    "bucket = 'c137242a3503187l8793420t1w504868800693-labbucket-fd7g4wutxall'\n",
    "data_access_role_arn = 'arn:aws:iam::504868800693:role/service-role/c137242a3503187l8793420t1w-ComprehendDataAccessRole-8283gGw7mpcK'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the newsgroup files\n",
    "([Go to top](#Lab-6.1:-Implementing-Topic-Modeling-with-Amazon-Comprehend))\n",
    "\n",
    "Now define the folder to hold the data. Then, clean up the folder, which might contain data from previous experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "data_dir = '20_newsgroups'\n",
    "if os.path.exists(data_dir):  # Clean up existing data folder\n",
    "    shutil.rmtree(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf ../s3/20_newsgroups.tar.gz\n",
    "!ls 20_newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [os.path.join(data_dir,f) for f in sorted(os.listdir(data_dir)) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "file_list = [os.path.join(d,f) for d in folders for f in os.listdir(d)]\n",
    "print('Number of documents:', len(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining and preprocessing the data\n",
    "([Go to top](#Lab-6.1:-Implementing-Topic-Modeling-with-Amazon-Comprehend))\n",
    "    \n",
    "In this section, you will examine the data and perform some standard natural language processing (NLP) data cleaning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 20_newsgroups/comp.graphics/37917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\n",
    "import re\n",
    "def strip_newsgroup_header(text):\n",
    "    \"\"\"\n",
    "    Given text in \"news\" format, strip the headers by removing everything\n",
    "    before the first blank line.\n",
    "    \"\"\"\n",
    "    _before, _blankline, after = text.partition('\\n\\n')\n",
    "    return after\n",
    "\n",
    "_QUOTE_RE = re.compile(r'(writes in|writes:|wrote:|says:|said:'\n",
    "                       r'|^In article|^Quoted from|^\\||^>)')\n",
    "\n",
    "\n",
    "def strip_newsgroup_quoting(text):\n",
    "    \"\"\"\n",
    "    Given text in \"news\" format, strip lines beginning with the quote\n",
    "    characters > or |, plus lines that often introduce a quoted section\n",
    "    (for example, because they contain the string 'writes:'.)\n",
    "    \"\"\"\n",
    "    good_lines = [line for line in text.split('\\n')\n",
    "                  if not _QUOTE_RE.search(line)]\n",
    "    return '\\n'.join(good_lines)\n",
    "\n",
    "\n",
    "def strip_newsgroup_footer(text):\n",
    "    \"\"\"\n",
    "    Given text in \"news\" format, attempt to remove a signature block.\n",
    "\n",
    "    As a rough heuristic, we assume that signatures are set apart by either\n",
    "    a blank line or a line made of hyphens, and that it is the last such line\n",
    "    in the file (disregarding blank lines at the end).\n",
    "    \"\"\"\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line_num in range(len(lines) - 1, -1, -1):\n",
    "        line = lines[line_num]\n",
    "        if line.strip().strip('-') == '':\n",
    "            break\n",
    "\n",
    "    if line_num > 0:\n",
    "        return '\\n'.join(lines[:line_num])\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, save all of the newsgroup documents to a single file, with one document on each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comprehend_input.txt','w', encoding='UTF-8') as cf:\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        line = re.sub('\\n',' ',line)\n",
    "        line = re.sub('\\r',' ',line)\n",
    "        cf.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).upload_file('comprehend_input.txt', 'comprehend/newsgroups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 20\n",
    "\n",
    "input_s3_url = f\"s3://{bucket}/comprehend\"\n",
    "input_doc_format = \"ONE_DOC_PER_LINE\"\n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "\n",
    "output_s3_url = f\"s3://{bucket}/outputfolder/\"\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    "\n",
    "job_uuid = uuid.uuid1()\n",
    "job_name = f\"top-job-{job_uuid}\"\n",
    "\n",
    "print(input_s3_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current job status\n",
    "from time import sleep\n",
    "job = comprehend_client.describe_topics_detection_job(JobId=start_topics_detection_job_result['JobId'])\n",
    "\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 40\n",
    "while job['TopicsDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(60)\n",
    "    waited += 60\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    print('.', end='')\n",
    "    job = comprehend_client.describe_topics_detection_job(JobId=start_topics_detection_job_result['JobId'])\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the .tar file\n",
    "import tarfile\n",
    "tf = tarfile.open('output.tar.gz')\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Analyzing the Amazon Comprehend Events output\n",
    "([Go to top](#Lab-6.1:-Implementing-Topic-Modeling-with-Amazon-Comprehend))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dftopicterms = pd.read_csv(\"topic-terms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting rows based on condition\n",
    "for t in range(0,number_of_topics):\n",
    "    rslt_df = dftopicterms.loc[dftopicterms['topic'] == t]\n",
    "    topic_list = rslt_df['term'].values.tolist()\n",
    "    print(f'Topic {t:2} - {topic_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = pd.DataFrame({'topics':['topic 0', 'topic 1', 'topic 2', 'topic 3', 'topic 4', 'topic 5', 'topic 6','topic 7','topic 8','topic 9',\n",
    "       'topic 10', 'topic 11', 'topic 12', 'topic 13', 'topic 14', 'topic 15', 'topic 16','topic 17','topic 18','topic 19']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdoctopics = pd.read_csv(\"doc-topics.csv\")\n",
    "dfdoctopics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_chart = dfdoctopics.loc[dfdoctopics['docname'].isin(['newsgroups:1000','newsgroups:2000','newsgroups:3000','newsgroups:4000','newsgroups:5000'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "# df.index = colnames['topic']\n",
    "to_chart.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
