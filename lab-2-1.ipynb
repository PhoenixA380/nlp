{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install --upgrade botocore\n",
    "!pip install --upgrade awscli\n",
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first five rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the options in the notebook to display more of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at specific entries if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[[580]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to know what data types you are dealing with. You can use `dtypes` on the dataframe to display the types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performing exploratory data analysis\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "You will now look at the target distribution for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({0:1, 1:0})\n",
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text processing: Removing stopwords and stemming\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get a list of stopwords from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for your problem. You don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', 'don\\'t','ain', 'are', 'aren\\'t', 'could', 'couldn\\'t',\n",
    "             'did', 'didn\\'t', 'does', 'doesn\\'t', 'had', 'hadn\\'t', 'has', 'hasn\\'t', \n",
    "             'have', 'haven\\'t', 'is', 'isn\\'t', 'might', 'mightn\\'t', 'must', 'mustn\\'t',\n",
    "             'need', 'needn\\'t','should', 'shouldn\\'t', 'was', 'wasn\\'t', 'were', \n",
    "             'weren\\'t', 'won\\'t', 'would', 'wouldn\\'t']\n",
    "\n",
    "# New stopword list\n",
    "stopwords = [word for word in stop if word not in excluding]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        \n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = ''\n",
    "            \n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # Applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stopwords\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) # Final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "        \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting training, validation, and test data\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[['reviewText', 'summary', 'time', 'log_votes']],\n",
    "                                                  df['isPositive'],\n",
    "                                                  test_size=0.20,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=324\n",
    "                                                 )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val,\n",
    "                                                y_val,\n",
    "                                                test_size=0.5,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset split, you can now run the `process_text` function defined above on each of the text features in the training, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing the reviewText fields')\n",
    "X_train['reviewText'] = process_text(X_train['reviewText'].tolist())\n",
    "X_val['reviewText'] = process_text(X_val['reviewText'].tolist())\n",
    "X_test['reviewText'] = process_text(X_test['reviewText'].tolist())\n",
    "\n",
    "print('Processing the summary fields')\n",
    "X_train['summary'] = process_text(X_train['summary'].tolist())\n",
    "X_val['summary'] = process_text(X_val['summary'].tolist())\n",
    "X_test['summary'] = process_text(X_test['summary'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing data with pipelines and a ColumnTransformer\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "numerical_features = ['time',\n",
    "                      'log_votes']\n",
    "\n",
    "text_features = ['summary',\n",
    "                 'reviewText']\n",
    "\n",
    "model_features = numerical_features + text_features\n",
    "model_target = 'isPositive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) \n",
    "                                ])\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "                                    ]) \n",
    "\n",
    "### DATA PREPROCESSING ###\n",
    "##########################\n",
    "\n",
    "print('Datasets shapes before processing: ', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "X_train = data_preprocessor.fit_transform(X_train).toarray()\n",
    "X_val = data_preprocessor.transform(X_val).toarray()\n",
    "X_test = data_preprocessor.transform(X_test).toarray()\n",
    "\n",
    "print('Datasets shapes after processing: ', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the number of features in the datasets went from 4 to 202."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a classifier with a built-in SageMaker algorithm\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Call the LinearLearner estimator object\n",
    "linear_classifier = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                           instance_count=1,\n",
    "                                           instance_type='ml.m4.xlarge',\n",
    "                                           predictor_type='binary_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the training, validation, and test parts of the estimator, you can use the `record_set()` function of the `binary_estimator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = linear_classifier.record_set(X_train.astype('float32'),\n",
    "                                            y_train.values.astype('float32'),\n",
    "                                            channel='train')\n",
    "val_records = linear_classifier.record_set(X_val.astype('float32'),\n",
    "                                          y_val.values.astype('float32'),\n",
    "                                          channel='validation')\n",
    "test_records = linear_classifier.record_set(X_test.astype('float32'),\n",
    "                                           y_test.values.astype('float32'),\n",
    "                                           channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` function applies a distributed version of the Stochastic Gradient Descent (SGD) algorithm, and you are sending the data to it. The logs were disabled with `logs=False`. You can remove that parameter to see more details about the process. __This process takes about 3-4 minutes on an ml.m4.xlarge instance.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier.fit([train_records,\n",
    "                       val_records,\n",
    "                       test_records],\n",
    "                       logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the model\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(linear_classifier._current_job_name, \n",
    "                                         metric_names = ['test:binary_classification_accuracy']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploying the model to an endpoint\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_classifier_predictor = linear_classifier.deploy(initial_instance_count = 1,\n",
    "                                                       instance_type = 'ml.c5.large'\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing the endpoint\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get test data in batch size of 25 and make predictions.\n",
    "prediction_batches = [linear_classifier_predictor.predict(batch)\n",
    "                      for batch in np.array_split(X_test.astype('float32'), 25)\n",
    "                     ]\n",
    "\n",
    "# Get a list of predictions\n",
    "print([pred.label['score'].float32_tensor.values[0] for pred in prediction_batches[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleaning up model artifacts\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "12bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
